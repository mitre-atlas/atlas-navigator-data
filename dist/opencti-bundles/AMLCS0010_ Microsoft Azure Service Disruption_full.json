{
    "type": "bundle",
    "id": "bundle--ce0e48fe-3174-4e6d-9887-d87f2b286f63",
    "objects": [
        {
            "id": "identity--48b4f20a-ddd3-5524-a646-55dd68b62ede",
            "spec_version": "2.1",
            "identity_class": "organization",
            "name": "MITRE-ATLAS",
            "created": "2024-02-06T15:26:40.634Z",
            "modified": "2024-02-06T15:26:40.781Z",
            "x_opencti_id": "7dd82baf-2bc3-402c-81a1-6a871929a136",
            "x_opencti_type": "Organization",
            "type": "identity"
        },
        {
            "type": "marking-definition",
            "spec_version": "2.1",
            "id": "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
            "created": "2017-01-20T00:00:00.000Z",
            "definition_type": "tlp",
            "name": "TLP:CLEAR",
            "definition": {
                "tlp": "clear"
            }
        },
        {
            "id": "report--2a5fab52-5c12-56b2-8ca2-5e1e3c8ae6bb",
            "spec_version": "2.1",
            "revoked": false,
            "x_opencti_reliability": "A - Completely reliable",
            "confidence": 75,
            "created": "2020-01-12T12:00:00.000Z",
            "modified": "2024-05-22T17:24:46.467Z",
            "name": "MITRE ATLAS Case Study: Microsoft Azure Service Disruption",
            "description": "The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding valid account, and exfiltrating data -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",
            "report_types": [
                "internal-report"
            ],
            "published": "2020-01-12T12:00:00.000Z",
            "x_opencti_workflow_id": "4c154301-4863-42d1-ba1a-bcf5cff32385",
            "labels": [
                "mitre atlas source",
                "aml.cs0010"
            ],
            "external_references": [
                {
                    "source_name": "AML.CS0010",
                    "url": "https://atlas.mitre.org/studies/AML.CS0010/"
                }
            ],
            "x_opencti_id": "4f8938d4-2da4-445b-8770-5f7f18cd9248",
            "x_opencti_type": "Report",
            "type": "report",
            "created_by_ref": "identity--48b4f20a-ddd3-5524-a646-55dd68b62ede",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9"
            ],
            "object_refs": [
                "attack-pattern--18c33065-849f-5705-b4d4-28a08470fba6",
                "attack-pattern--2dc305cb-dd66-55b6-a0c8-87dbf69bae66",
                "attack-pattern--4c320cfb-4618-5a63-9b63-dce97f905381",
                "attack-pattern--55e19f1d-312b-5980-9872-10557f9dab27",
                "attack-pattern--d9a8d291-0c88-5ee8-a87d-71d7f0769d64",
                "attack-pattern--e05ef699-0400-5c8e-b52f-37407175b1f5",
                "attack-pattern--e4fbace6-e94f-58f4-aed8-a7ea2f19a213",
                "attack-pattern--f8fdadaa-41e5-557e-a2e7-ea045e679d8d",
                "incident--8458a7d3-dc00-5460-9d32-8db303ab7415"
            ]
        },
        {
            "type": "marking-definition",
            "spec_version": "2.1",
            "id": "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101",
            "created": "2017-06-01T00:00:00.000Z",
            "definition_type": "statement",
            "name": "Copyright 2015-2023, The MITRE Corporation. MITRE ATT&CK and ATT&CK are registered trademarks of The MITRE Corporation.",
            "definition": {
                "statement": "copyright 2015-2023, the mitre corporation. mitre att&ck and att&ck are registered trademarks of the mitre corporation."
            }
        },
        {
            "id": "attack-pattern--18c33065-849f-5705-b4d4-28a08470fba6",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.631Z",
            "modified": "2023-11-20T18:19:47.016Z",
            "name": "Exfiltration via Cyber Means",
            "description": "Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means.\n\nSee the ATT&CK [Exfiltration](https://attack.mitre.org/tactics/TA0010/) tactic for more information.",
            "x_mitre_id": "AML.T0025",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "exfiltration",
                    "x_opencti_order": 13
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0025",
                    "external_id": "AML.T0025"
                }
            ],
            "x_opencti_id": "ec43ad24-eebc-4442-b1cd-7d82123b9313",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "attack-pattern--2dc305cb-dd66-55b6-a0c8-87dbf69bae66",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.628Z",
            "modified": "2023-11-20T18:19:43.967Z",
            "name": "Valid Accounts",
            "description": "Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.\nCredentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.\n\nCompromised credentials may provide access to additional ML artifacts and allow the adversary to perform [Discover ML Artifacts](/techniques/AML.T0007).\nCompromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.",
            "x_mitre_id": "AML.T0012",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access",
                    "x_opencti_order": 3
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0012",
                    "external_id": "AML.T0012"
                }
            ],
            "x_opencti_id": "eb0a6b17-ae24-4d15-be8f-c70ade49345d",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "attack-pattern--4c320cfb-4618-5a63-9b63-dce97f905381",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.633Z",
            "modified": "2023-11-20T18:19:48.833Z",
            "name": "White-Box Optimization",
            "description": "In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly.\nAdversarial examples trained in this manor are most effective against the target model.",
            "x_mitre_id": "AML.T0043.000",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging",
                    "x_opencti_order": 12
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0043.000",
                    "external_id": "AML.T0043.000"
                }
            ],
            "x_opencti_id": "5da3c325-7b20-44f8-85f2-5d709a0ca4bb",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "attack-pattern--55e19f1d-312b-5980-9872-10557f9dab27",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.624Z",
            "modified": "2023-11-20T18:19:40.982Z",
            "name": "ML Model Inference API Access",
            "description": "Adversaries may gain access to a model via legitimate access to the inference API.\nInference API access can be a source of information to the adversary ([Discover ML Model Ontology](/techniques/AML.T0013), [Discover ML Model Family](/techniques/AML.T0014)), a means of staging the attack ([Verify Attack](/techniques/AML.T0042), [Craft Adversarial Data](/techniques/AML.T0043)), or for introducing data to the target system for Impact ([Evade ML Model](/techniques/AML.T0015), [Erode ML Model Integrity](/techniques/AML.T0031)).",
            "x_mitre_id": "AML.T0040",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-model-access",
                    "x_opencti_order": 4
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0040",
                    "external_id": "AML.T0040"
                }
            ],
            "x_opencti_id": "14a799b7-27ff-4f71-b319-682cabf0e297",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "attack-pattern--d9a8d291-0c88-5ee8-a87d-71d7f0769d64",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.632Z",
            "modified": "2023-11-20T18:19:47.883Z",
            "name": "Verify Attack",
            "description": "Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.\nThis gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.\nThe adversary may verify the attack once but use it against many edge devices running copies of the target model.\nThe adversary may verify their attack digitally, then deploy it in the [Physical Environment Access](/techniques/AML.T0041) at a later time.\nVerifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.",
            "x_mitre_id": "AML.T0042",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "ml-attack-staging",
                    "x_opencti_order": 12
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0042",
                    "external_id": "AML.T0042"
                }
            ],
            "x_opencti_id": "0199c849-b9b8-4ac9-9101-4e73977e825a",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "attack-pattern--e05ef699-0400-5c8e-b52f-37407175b1f5",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.617Z",
            "modified": "2023-11-20T18:19:33.754Z",
            "name": "Search for Victim's Publicly Available Research Materials",
            "description": "Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.\nThe adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.\nOrganizations often use open source model architectures trained on additional proprietary data in production.\nKnowledge of this underlying architecture allows the adversary to craft more realistic proxy models ([Create Proxy ML Model](/techniques/AML.T0005)).\nAn adversary can search these resources for publications for authors employed at the victim organization.\n\nResearch materials may exist as academic papers published in [Journals and Conference Proceedings](/techniques/AML.T0000.000), or stored in [Pre-Print Repositories](/techniques/AML.T0000.001), as well as [Technical Blogs](/techniques/AML.T0000.002).",
            "x_mitre_id": "AML.T0000",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "reconnaissance",
                    "x_opencti_order": 1
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0000",
                    "external_id": "AML.T0000"
                }
            ],
            "x_opencti_id": "213bb1f4-b7ed-45d6-9620-f6e21bf50aca",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "attack-pattern--e4fbace6-e94f-58f4-aed8-a7ea2f19a213",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.628Z",
            "modified": "2023-11-20T18:19:45.409Z",
            "name": "Evade ML Model",
            "description": "Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevent a machine learning model from correctly identifying the contents of the data.\nThis technique can be used to evade a downstream task where machine learning is utilized.\nThe adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.",
            "x_mitre_id": "AML.T0015",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "defense-evasion",
                    "x_opencti_order": 8
                },
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "impact",
                    "x_opencti_order": 14
                },
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "initial-access",
                    "x_opencti_order": 3
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0015",
                    "external_id": "AML.T0015"
                }
            ],
            "x_opencti_id": "a1b49de3-3741-4137-a8b3-8c35c7713c28",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "attack-pattern--f8fdadaa-41e5-557e-a2e7-ea045e679d8d",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 0,
            "created": "2023-10-31T13:48:07.632Z",
            "modified": "2023-11-20T18:19:47.594Z",
            "name": "ML Artifact Collection",
            "description": "Adversaries may collect ML artifacts for [Exfiltration](/tactics/AML.TA0010) or for use in [ML Attack Staging](/tactics/AML.TA0001).\nML artifacts include models and datasets as well as other telemetry data produced when interacting with a model.",
            "x_mitre_id": "AML.T0035",
            "labels": [
                "atlas"
            ],
            "kill_chain_phases": [
                {
                    "kill_chain_name": "mitre-atlas",
                    "phase_name": "collection",
                    "x_opencti_order": 11
                }
            ],
            "external_references": [
                {
                    "source_name": "mitre-atlas",
                    "url": "https://atlas.mitre.org/techniques/AML.T0035",
                    "external_id": "AML.T0035"
                }
            ],
            "x_opencti_id": "ccfb1e6a-52ff-4997-bc9c-b8eb08fd69e4",
            "x_opencti_type": "Attack-Pattern",
            "type": "attack-pattern",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9",
                "marking-definition--c4099854-e00c-5dbc-b0fe-4c9909920101"
            ]
        },
        {
            "id": "incident--8458a7d3-dc00-5460-9d32-8db303ab7415",
            "spec_version": "2.1",
            "revoked": false,
            "confidence": 75,
            "created": "2024-02-06T15:29:37.168Z",
            "modified": "2024-02-06T16:44:38.985Z",
            "name": "2020 Microsoft Azure Service Disruption",
            "description": "The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding valid account, and exfiltrating data -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",
            "first_seen": "2020-01-12T12:00:00.000Z",
            "incident_type": "research-finding",
            "labels": [
                "aml.cs0010"
            ],
            "external_references": [
                {
                    "source_name": "AML.CS0010",
                    "url": "https://atlas.mitre.org/studies/AML.CS0010/"
                }
            ],
            "x_opencti_id": "7f59d9c4-f27e-4553-91be-788af57929c7",
            "x_opencti_type": "Incident",
            "type": "incident",
            "created_by_ref": "identity--48b4f20a-ddd3-5524-a646-55dd68b62ede",
            "object_marking_refs": [
                "marking-definition--613f2e26-407d-48c7-9eca-b8e91df99dc9"
            ]
        }
    ]
}