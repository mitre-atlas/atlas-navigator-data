{
    "versions": {
        "layer": "4.3",
        "navigator": "4.6.4"
    },
    "domain": "atlas-atlas",
    "metadata": [
        {
            "name": "url",
            "value": "https://atlas.mitre.org/studies/AML.CS0005"
        },
        {
            "name": "atlas_data_version",
            "value": "5.0.1"
        }
    ],
    "name": "Attack on Machine Translation Services",
    "description": "Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.\nA research group at UC Berkeley utilized these public endpoints to create a replicated model with near-production state-of-the-art translation quality.\nBeyond demonstrating that IP can be functionally stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.\nThese adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.",
    "techniques": [
        {
            "techniqueID": "AML.T0000",
            "showSubtechniques": false,
            "tactic": "reconnaissance",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0002.000",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0002",
            "showSubtechniques": true,
            "tactic": "resource-development"
        },
        {
            "techniqueID": "AML.T0002.001",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0002",
            "showSubtechniques": true,
            "tactic": "resource-development"
        },
        {
            "techniqueID": "AML.T0040",
            "showSubtechniques": false,
            "tactic": "ai-model-access",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0005.001",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0005",
            "showSubtechniques": true,
            "tactic": "ai-attack-staging"
        },
        {
            "techniqueID": "AML.T0048.004",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0048",
            "showSubtechniques": true,
            "tactic": "impact"
        },
        {
            "techniqueID": "AML.T0043.002",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0043",
            "showSubtechniques": true,
            "tactic": "ai-attack-staging"
        },
        {
            "techniqueID": "AML.T0015",
            "showSubtechniques": false,
            "tactic": "impact",
            "color": "#C8E6C9"
        },
        {
            "techniqueID": "AML.T0031",
            "showSubtechniques": false,
            "tactic": "impact",
            "color": "#C8E6C9"
        }
    ],
    "legendItems": [
        {
            "label": "Used in case study",
            "color": "#C8E6C9"
        }
    ]
}